{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kz1d-dBkgN5l"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import dok_matrix, coo_matrix\n",
    "from sklearn.utils.multiclass import  type_of_target\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHHU4AeNUGn4"
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q10LcyhtT91-"
   },
   "outputs": [],
   "source": [
    "# Ref: https://github.com/sh1ng/imba/blob/master/Product2VecSkipGram.py\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import reduce\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "class Product2VecSkipGram:\n",
    "    def __init__(self, data, cv_data, batch_size, num_skips, skip_window,\n",
    "                 vocabulary_size, embedding_size=32,\n",
    "                 num_negative_sampled=64, len_ratio = 0.5):\n",
    "        self.data = data\n",
    "        self.cv_data = cv_data\n",
    "        self.data_index = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.num_skips = num_skips\n",
    "        self.skip_window = skip_window\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_negative_sampled = num_negative_sampled\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.len_ratio = len_ratio\n",
    "        assert batch_size % num_skips == 0\n",
    "        assert num_skips <= 2 * skip_window\n",
    "        self.build_graph()\n",
    "\n",
    "    def predict(self, products):\n",
    "        result = []\n",
    "        for i in range(0, len(products), self.batch_size):\n",
    "            batch = products[i:i+self.batch_size]\n",
    "            batch = self.sess.run(self.gathered, feed_dict={self.train_inputs: batch})\n",
    "            result.append(batch)\n",
    "        return np.concatenate(result, axis=0)\n",
    "\n",
    "    def train(self, num_steps, cv_every_n_steps, cv_steps, lrs):\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            average_loss = 0\n",
    "            learning_rate = 1.0\n",
    "            current = executor.submit(self.generate_batch)\n",
    "            for step in range(num_steps):\n",
    "                if step in lrs:\n",
    "                    learning_rate = lrs[step]\n",
    "                batch_inputs, batch_labels = current.result()\n",
    "                current = executor.submit(self.generate_batch)\n",
    "                feed_dict = {self.train_inputs: batch_inputs,\n",
    "                             self.train_labels: batch_labels,\n",
    "                             self.learning_rate: learning_rate}\n",
    "\n",
    "                _, loss_val = self.sess.run([self.optimizer, self.loss], feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "\n",
    "                if step % 2000 == 0:\n",
    "                    if step > 0:\n",
    "                        average_loss /= 2000\n",
    "                    print('Average loss at step ', step, ': ', average_loss)\n",
    "                    average_loss = 0\n",
    "                if step % cv_every_n_steps == 0:\n",
    "                    self.data = shuffle(self.data, random_state=0)\n",
    "                    self.save_model(step)\n",
    "                    cv_loss = 0\n",
    "                    for batch_inputs, batch_labels in self.generate_test(cv_steps):\n",
    "                        feed_dict = {self.train_inputs: batch_inputs,\n",
    "                                     self.train_labels: batch_labels,\n",
    "                                     self.learning_rate: learning_rate}\n",
    "                        loss_val = self.sess.run(self.loss, feed_dict=feed_dict)\n",
    "                        cv_loss += loss_val\n",
    "                    print('CV',cv_loss / cv_steps)\n",
    "\n",
    "    def save_model(self, step):\n",
    "        self.saver.save(self.sess, 'models/prod2vec_skip_gram', global_step=step)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.saver.restore(self.sess, path)\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.train_inputs = tf.compat.v1.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.train_labels = tf.compat.v1.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.learning_rate = tf.compat.v1.placeholder(tf.float32)\n",
    "\n",
    "        # variables\n",
    "        embeddings = tf.Variable(tf.random.uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0))\n",
    "\n",
    "        softmax_weights = tf.Variable(tf.random.truncated_normal([self.embedding_size, self.vocabulary_size],\n",
    "                                                          stddev=1.0 / math.sqrt(self.embedding_size)))\n",
    "        softmax_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "\n",
    "        self.gathered = tf.gather(embeddings, self.train_inputs)\n",
    "\n",
    "        prediction = tf.matmul(self.gathered, softmax_weights) + softmax_biases\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.train_labels, logits=prediction))\n",
    "\n",
    "        self.optimizer = tf.compat.v1.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        self.sess = tf.compat.v1.Session()\n",
    "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        self.saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "\n",
    "    def inc(self):\n",
    "        self.data_index = (self.data_index + 1) % len(self.data)\n",
    "\n",
    "    def inc_cv(self, data_index):\n",
    "        return (data_index + 1) % len(self.cv_data)\n",
    "\n",
    "    def generate_batch(self):\n",
    "        batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        counter = 0\n",
    "        while counter < self.batch_size:\n",
    "            current = self.data.iloc[self.data_index]\n",
    "            if len(current) == 1:\n",
    "                warnings.warn(\"lenght is one\", RuntimeWarning)\n",
    "                self.inc()\n",
    "                continue\n",
    "\n",
    "            span = min(2 * self.skip_window + 1, len(current))\n",
    "\n",
    "            x = target = np.random.randint(0, len(current))\n",
    "\n",
    "            targets_to_avoid = [x]\n",
    "\n",
    "            for j in range(self.num_skips):  # target varies!!! X constant!\n",
    "                while target in targets_to_avoid and len(targets_to_avoid) != span:\n",
    "                    target = np.random.randint(0, span)\n",
    "                if len(targets_to_avoid) == span or counter == self.batch_size:\n",
    "                    break\n",
    "                targets_to_avoid.append(target)\n",
    "                batch[counter] = current[x]\n",
    "                labels[counter] = current[target]\n",
    "                counter += 1\n",
    "            self.inc()\n",
    "\n",
    "        return batch, labels\n",
    "\n",
    "    def generate_test(self, num_steps):\n",
    "        data_index = 0\n",
    "        for _ in range(num_steps):\n",
    "            batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "            labels = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "\n",
    "            counter = 0\n",
    "            while counter < self.batch_size:\n",
    "                current = self.cv_data.iloc[data_index]\n",
    "                if len(current) == 1:\n",
    "                    warnings.warn(\"lenght is one\", RuntimeWarning)\n",
    "                    data_index = self.inc_cv(data_index)\n",
    "                    continue\n",
    "\n",
    "                span = min(2 * self.skip_window + 1, len(current))\n",
    "\n",
    "                x = target = np.random.randint(0, len(current))\n",
    "\n",
    "                targets_to_avoid = [x]\n",
    "\n",
    "                for j in range(self.num_skips):  # target varies!!! X constant!\n",
    "                    while target in targets_to_avoid and len(targets_to_avoid) != span:\n",
    "                        target = np.random.randint(0, span)\n",
    "                    if len(targets_to_avoid) == span or counter == self.batch_size:\n",
    "                        break\n",
    "                    targets_to_avoid.append(target)\n",
    "                    batch[counter] = current[x]\n",
    "                    labels[counter] = current[target]\n",
    "                    counter += 1\n",
    "                data_index = self.inc_cv(data_index)\n",
    "\n",
    "            yield batch, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKUKw7xIg_zA"
   },
   "source": [
    "# Get the products purchased by the user till now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUfLleoOgdtb"
   },
   "outputs": [],
   "source": [
    "order_prior = pd.read_csv(\"data/order_products__prior.csv\")\n",
    "orders = pd.read_csv(\"data/orders.csv\")\n",
    "\n",
    "# Get all the Prior orders.\n",
    "orders = orders[orders.eval_set == 'prior']\n",
    "\n",
    "# Get the order_id and its respective user_id.\n",
    "orders_user = orders[['order_id', 'user_id']]\n",
    "\n",
    "# Merge the prior_orders with order_user in order to get the user who purchased\n",
    "# a respective product(s).\n",
    "user_product = pd.merge(order_prior, orders_user, on='order_id')\n",
    "\n",
    "# Fetch the user_id and product_id and remove duplicates so we can get what\n",
    "# unique product was bought by a given user.\n",
    "user_product = user_product.loc[:, ['user_id', 'product_id']].drop_duplicates()\n",
    "\n",
    "# Save the user purchased products for further use.\n",
    "user_product.to_pickle('data/previous_products.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exf3uYD1hC1g"
   },
   "source": [
    "# link each user's order with product bought by till now and divide them in train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rb86MCpxhE36"
   },
   "outputs": [],
   "source": [
    "order_train = pd.read_csv(\"data/order_products__train.csv\",\n",
    "                          dtype={'order_id': np.uint32,\n",
    "                                 'product_id': np.uint16,\n",
    "                                 'reordered': bool})\n",
    "\n",
    "orders = pd.read_csv(\"data/orders.csv\")\n",
    "\n",
    "user_product = pd.read_pickle('data/previous_products.pkl')\n",
    "\n",
    "# Get all the orders except for priors from the orders.csv\n",
    "orders = orders.loc[(orders.eval_set=='train') | (orders.eval_set=='test'), :]\n",
    "\n",
    "# Connect the product_id and order_id based on the eval_set\n",
    "user_product = pd.merge(\n",
    "    user_product, orders[['order_id', 'user_id', 'eval_set']],\n",
    "    on='user_id').drop(['user_id'], axis=1)\n",
    "\n",
    "order_train.drop(['add_to_cart_order'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Merge the train orders based on order_id and product_id to get the r\n",
    "# reorderd bool value and eval_set for given order and product id\n",
    "current = pd.merge(order_train, user_product,\n",
    "               on=['order_id', 'product_id'], how='right')\n",
    "current.reordered.fillna(False, inplace=True)\n",
    "print(current.shape)\n",
    "# Save the chunk of the data\n",
    "current.to_pickle('data/train_test_set.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtKYn3I4OQj1"
   },
   "source": [
    "# User Product Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-6SiUuiKu2j"
   },
   "outputs": [],
   "source": [
    "order_prior = pd.read_csv(\"data/order_products__prior.csv\",\n",
    "                          dtype={\n",
    "                              'order_id': np.uint32,\n",
    "                              'product_id': np.uint16,\n",
    "                              'add_to_cart_order': np.uint8,\n",
    "                              'reordered': bool}\n",
    "                          )\n",
    "order_train = pd.read_csv(\"data/order_products__train.csv\",\n",
    "                          dtype={\n",
    "                              'order_id': np.uint32,\n",
    "                              'product_id': np.uint16,\n",
    "                              'add_to_cart_order': np.uint8,\n",
    "                              'reordered': bool})\n",
    "orders = pd.read_csv(\"data/orders.csv\")\n",
    "\n",
    "products = pd.read_csv(\"data/products.csv\")\n",
    "\n",
    "order_train = pd.read_pickle('data/train_test_set.pkl')\n",
    "\n",
    "# Get the ordered products with corresponding order_id\n",
    "orders_products = pd.merge(orders, order_prior, on=\"order_id\")\n",
    "\n",
    "# Get the department and aisle id of the respective product\n",
    "orders_products_products = pd.merge(\n",
    "    orders_products,\n",
    "    products[['product_id', 'department_id', 'aisle_id']],\n",
    "    on='product_id')\n",
    "\n",
    "# get the unique products bought by user from a given department and\n",
    "# sum of reorder count\n",
    "user_dep_stat = orders_products_products.groupby(\n",
    "    ['user_id', 'department_id']).agg(\n",
    "        {\n",
    "        'product_id': lambda x: x.nunique(),\n",
    "        'reordered': 'sum'\n",
    "        })\n",
    "# Rename the columns\n",
    "user_dep_stat.rename(columns={'product_id': 'dep_products',\n",
    "                                'reordered': 'dep_reordered'}, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "user_dep_stat.reset_index(inplace=True)\n",
    "user_dep_stat.to_pickle('data/user_department_products.pkl')\n",
    "\n",
    "# Perform same steps for aisle and get the products purchased from given aisle\n",
    "# and its reorder sum\n",
    "user_aisle_stat = orders_products_products.groupby(\n",
    "    ['user_id', 'aisle_id']).agg(\n",
    "        {\n",
    "         'product_id': lambda x: x.nunique(),\n",
    "         'reordered': 'sum'\n",
    "         })\n",
    "user_aisle_stat.rename(columns={'product_id': 'aisle_products',\n",
    "                                'reordered': 'aisle_reordered'}, inplace=True)\n",
    "user_aisle_stat.reset_index(inplace=True)\n",
    "user_aisle_stat.to_pickle('data/user_aisle_products.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cm90NpD9j2jc"
   },
   "outputs": [],
   "source": [
    "aggregated = temp.copy()\n",
    "# Get the last and second last period of the shopping and calculate the mean,\n",
    "# median of the periods\n",
    "aggregated['last'] = aggregated.periods.apply(lambda x: x[-1])\n",
    "aggregated['prev1'] = aggregated.periods.apply(\n",
    "    lambda x: x[-2] if len(x) > 1 else np.nan)\n",
    "aggregated['prev2'] = aggregated.periods.apply(\n",
    "    lambda x: x[-3] if len(x) > 2 else np.nan)\n",
    "aggregated['median'] = aggregated.periods.apply(lambda x: np.median(x[:-1]))\n",
    "aggregated['mean'] = aggregated.periods.apply(lambda x: np.mean(x[:-1]))\n",
    "aggregated.drop('periods', axis=1, inplace=True)\n",
    "\n",
    "aggregated.to_pickle('data/product_periods_stat.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxLNe8yFjP05"
   },
   "source": [
    "# Calculate the cumulative sum of order's days_since_prior_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zk3dqZbDjRV1"
   },
   "outputs": [],
   "source": [
    "order_prior = pd.read_csv(\n",
    "    \"data/order_products__prior.csv\",\n",
    "    dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16,\n",
    "           'add_to_cart_order': np.uint8,\n",
    "           'reordered': bool})\n",
    "order_train = pd.read_csv(\n",
    "    \"data/order_products__train.csv\",\n",
    "    dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16,\n",
    "           'add_to_cart_order': np.uint8,\n",
    "           'reordered': bool})\n",
    "\n",
    "orders = pd.read_csv(\"data/orders.csv\")\n",
    "\n",
    "labels = pd.read_pickle('data/train_test_set.pkl')\n",
    "user_product = pd.read_pickle('data/previous_products.pkl')\n",
    "\n",
    "# Calculate the cumulative sum of the day_since_prior_order\n",
    "order_cumsum = orders[\n",
    "                      ['user_id', 'order_number',\n",
    "                       'days_since_prior_order']\n",
    "                      ].groupby(['user_id', 'order_number']).agg({\n",
    "                          'days_since_prior_order': 'sum'\n",
    "                      })\n",
    "order_cumsum = order_cumsum.groupby(level=0).cumsum().reset_index()\n",
    "order_cumsum = order_cumsum.rename(\n",
    "    columns={'days_since_prior_order':'days_since_prior_order_cumsum'})\n",
    "\n",
    "\n",
    "order_cumsum.to_pickle('data/order_cumsum.pkl')\n",
    "print(\"saved order_cumsum\")\n",
    "\n",
    "# Merge it with orders to get the order_id w.r.t days_since_prior_order\n",
    "order_cumsum = pd.merge(\n",
    "    order_cumsum,\n",
    "    orders, on=['user_id', 'order_number']\n",
    "    )[['user_id', 'order_number', 'days_since_prior_order_cumsum', 'order_id']]\n",
    "\n",
    "# Get the products bought in prior orders along with its order_id\n",
    "order_product = pd.merge(\n",
    "    order_prior, orders, on='order_id'\n",
    "    )[['order_id', 'product_id', 'eval_set']]\n",
    "\n",
    "# Get the test and train order and product_id\n",
    "order_product_train_test = labels[['order_id', 'product_id', 'eval_set']]\n",
    "\n",
    "# merge the prior and prior,train and test orders along with its product_id\n",
    "order_product = pd.concat([order_product, order_product_train_test])\n",
    "\n",
    "order_product = pd.merge(order_product, order_cumsum, on='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xigQsbklxfoe"
   },
   "outputs": [],
   "source": [
    "temp = order_product.groupby(['user_id', 'product_id', 'order_number']).agg({\n",
    "    'days_since_prior_order_cumsum':'sum'\n",
    "    })\n",
    "print(f\"1st part done. {temp}\")\n",
    "temp = temp.astype(np.float16)\n",
    "temp = temp.groupby(level=[0, 1])\n",
    "print(f\"2nd part done {temp.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRlyMZNoB7iU"
   },
   "outputs": [],
   "source": [
    "temp = temp.apply(lambda x: np.diff(np.nan_to_num(x)))\n",
    "temp = temp.to_frame('periods').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab80w5F-SvJU"
   },
   "source": [
    "# Prod2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDzldMhARV60"
   },
   "outputs": [],
   "source": [
    "order_prior = pd.read_csv(\"data/order_products__prior.csv\")\n",
    "orders = pd.read_csv(\"data/orders.csv\")\n",
    "\n",
    "data = pd.merge(order_prior, orders, on='order_id')\n",
    "\n",
    "data = order_prior.sort_values(['order_id']).groupby('order_id')['product_id']\\\n",
    "    .apply(lambda x: x.tolist()).to_frame('products').reset_index()\n",
    "data = pd.merge(data, orders, on='order_id')\n",
    "data.to_pickle('data/prod2vec.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2iQShVSUJc-"
   },
   "source": [
    "# Skip gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCziPILzSyJZ"
   },
   "outputs": [],
   "source": [
    "# Ref: https://github.com/sh1ng/imba/blob/master/skip_gram_train.py\n",
    "np.random.seed(2017)\n",
    "products = pd.read_csv('data/instacart/products.csv')\n",
    "df = pd.read_pickle('data/prod2vec.pkl').products\n",
    "print('initial size', len(df))\n",
    "\n",
    "df_train, df_cv = train_test_split(df, test_size=0.1, random_state=2017)\n",
    "batch_size = 1024\n",
    "rates = {\n",
    "    100000: 0.5,\n",
    "    200000: 0.25,\n",
    "    500000: 0.1\n",
    "    }\n",
    "model = Product2VecSkipGram(df_train, df_cv, batch_size,\n",
    "                            1, 1, np.max(products.product_id) + 1)\n",
    "model.train(120001, 20000, len(df_cv) // batch_size, rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-35xjkv8UVRl"
   },
   "outputs": [],
   "source": [
    "# Ref: https://github.com/sh1ng/imba/blob/master/skip_gram_get.py\n",
    "np.random.seed(2017)\n",
    "products = pd.read_csv('data/instacart/products.csv')\n",
    "df = pd.read_pickle(\n",
    "    'data/prod2vec.pkl').products.tolist()\n",
    "print('initial size', len(df))\n",
    "\n",
    "df_train, df_cv = train_test_split(df, test_size = 0.1, random_state=2017)\n",
    "batch_size = 128\n",
    "rates = {100000: 0.5,\n",
    "         200000: 0.25,\n",
    "         500000: 0.1}\n",
    "model = Product2VecSkipGram(df_train, df_cv, len(products),\n",
    "                            1, 1, np.max(products.product_id) + 1)\n",
    "model.sess = tf.compat.v1.Session()\n",
    "model.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "model.saver = tf.compat.v1.train.Saver()\n",
    "model.load_model('models/prod2vec_skip_gram-120000')\n",
    "embd = model.predict(products.product_id.values)\n",
    "products = pd.concat([products, pd.DataFrame(embd)], axis=1)\n",
    "products.to_pickle('data/product_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVfwbDaG2dkE"
   },
   "source": [
    "# Prepare order streak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8NBZngU2fEM"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Faron\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "Calculates (user, product) order_streak for the last n orders.\n",
    "\n",
    "- abs(order_streak) is length of streak\n",
    "- sgn(order_streak) encodes type of streak (non-ordered vs ordered)\n",
    "'''\n",
    "\n",
    "DATA_DIR = \"data/\"\n",
    "PRIOR_FILE = \"order_products__prior\"\n",
    "ORDERS_FILE = \"orders\"\n",
    "\n",
    "\n",
    "def load_input_data():\n",
    "    PATH = \"{}{}{}\".format(DATA_DIR, PRIOR_FILE, \".csv\")\n",
    "    prior = pd.read_csv(PATH)\n",
    "\n",
    "    PATH = \"{}{}{}\".format(DATA_DIR, ORDERS_FILE, \".csv\")\n",
    "    orders = pd.read_csv(PATH)\n",
    "    return prior, orders\n",
    "\n",
    "\n",
    "def apply_parallel(df_groups, _func):\n",
    "    nthreads = multiprocessing.cpu_count() >> 1\n",
    "    print(\"nthreads: {}\".format(nthreads))\n",
    "\n",
    "    res = Parallel(n_jobs=nthreads)(delayed(_func)(grp.copy()) for _, grp in df_groups)\n",
    "    return pd.concat(res)\n",
    "\n",
    "\n",
    "def add_order_streak(df):\n",
    "    tmp = df.copy()\n",
    "    tmp.user_id = 1\n",
    "\n",
    "    UP = tmp.pivot(index=\"product_id\", columns='order_number').fillna(-1)\n",
    "    UP.columns = UP.columns.droplevel(0)\n",
    "\n",
    "    x = np.abs(UP.diff(axis=1).fillna(2)).values[:, ::-1]\n",
    "    df.set_index(\"product_id\", inplace=True)\n",
    "    df['order_streak'] = np.multiply(np.argmax(x, axis=1) + 1, UP.iloc[:, -1])\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "prior, orders = load_input_data()\n",
    "\n",
    "print(\"orders: {}\".format(orders.shape))\n",
    "print(\"take only recent 5 orders per user:\")\n",
    "orders = orders.groupby(['user_id']).tail(5 + 1)\n",
    "print(\"orders: {}\".format(orders.shape))\n",
    "\n",
    "prior = orders.merge(prior, how='inner', on=\"order_id\")\n",
    "prior = prior[['user_id', 'product_id', 'order_number']]\n",
    "print(\"prior: {}\".format(prior.shape))\n",
    "\n",
    "user_groups = prior.groupby('user_id')\n",
    "s = datetime.now()\n",
    "df = apply_parallel(user_groups, add_order_streak)\n",
    "e = datetime.now()\n",
    "print(\"time elapsed: {}\".format(e - s))\n",
    "\n",
    "df = df.drop(\"order_number\", axis=1).drop_duplicates().reset_index(drop=True)\n",
    "df = df[['user_id', 'product_id', 'order_streak']]\n",
    "df.to_csv(\"order_streaks.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GHHU4AeNUGn4"
   ],
   "name": "final data generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
