{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ-bM1Jl8PfX"
   },
   "source": [
    "# Load the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ywSH_gE94onQ"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import _pickle as cpickle\n",
    "import arboretum\n",
    "import lightgbm as lgb\n",
    "from operator import itemgetter\n",
    "import joblib\n",
    "import json\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import dok_matrix, coo_matrix\n",
    "from sklearn.utils.multiclass import  type_of_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3rqzVJN8Mad"
   },
   "source": [
    "# Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ygDIVRcf4y9X"
   },
   "outputs": [],
   "source": [
    "# Load the original files\n",
    "aisles = pd.read_csv(\"data/aisles.csv\",\n",
    "                     dtype={'aisle': 'category'})\n",
    "departments = pd.read_csv(\"data/departments.csv\",\n",
    "                          dtype={'department': 'category'})\n",
    "order_prior = pd.read_csv(\"data/order_products__prior.csv\",\n",
    "                          dtype={'reordered': bool})\n",
    "train_orders = pd.read_csv(\"data/order_products__train.csv\",\n",
    "                          dtype={'reordered': bool})\n",
    "orders = pd.read_csv(\"data/orders.csv\",\n",
    "                     dtype={'eval_set': 'category'})\n",
    "\n",
    "products = pd.read_csv(\"data/products.csv\")\n",
    "\n",
    "# Load the feature files created\n",
    "product_embeddings = pd.read_pickle('data/product_embeddings.pkl')\n",
    "order_train_chunk = pd.read_pickle(\"data/train_test_set.pkl\")\n",
    "product_periods = pd.read_pickle(\"data/product_periods_stat.pkl\").fillna(9999)\n",
    "user_dep_stat = pd.read_pickle(\"data/user_department_products.pkl\")\n",
    "user_aisle_stat = pd.read_pickle(\"data/user_aisle_products.pkl\")\n",
    "order_streaks = pd.read_csv(\"data/order_streaks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPZCLxNe8ZLv"
   },
   "source": [
    "# Get the train and test orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6XY-z1Pi5w_g"
   },
   "outputs": [],
   "source": [
    "order_test = order_train_chunk[order_train_chunk.eval_set == \"test\"][['order_id',\n",
    "                                                          'product_id']]\n",
    "order_train = order_train_chunk[order_train_chunk.eval_set == \"train\"][['order_id',\n",
    "                                                            'product_id',\n",
    "                                                            'reordered']]\n",
    "embedings = list(range(32))\n",
    "product_embeddings = product_embeddings[embedings + ['product_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycCT87rq9UWj"
   },
   "source": [
    "# Compute other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "C2WaUR1y8ghI"
   },
   "outputs": [],
   "source": [
    "prob = pd.merge(order_prior, orders, on='order_id')\n",
    "\n",
    "# Calculate the count of user ordered a given product and how many times\n",
    "# the product was reorderd by the user\n",
    "prob = prob.groupby(\n",
    "    ['product_id', 'user_id']).agg(\n",
    "    {\n",
    "    'reordered':'sum',\n",
    "    'user_id': 'size'\n",
    "    })\n",
    "\n",
    "# Rename the grouped columns\n",
    "prob.rename(\n",
    "    columns={'sum': 'reordered', 'user_id': 'total'},\n",
    "    inplace=True\n",
    "    )\n",
    "\n",
    "# Calculate the ratio of reorder\n",
    "prob['reorder_prob'] = prob.reordered / prob.total\n",
    "\n",
    "# Calculate the mean of the product reordered\n",
    "prob = prob.groupby('product_id').agg({'reorder_prob': 'mean'})\n",
    "prob = prob.rename(columns={'mean': 'reorder_prob'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrWZGsxMFhQO"
   },
   "source": [
    "## Calculate the product statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "r4x9SgfeFjYR"
   },
   "outputs": [],
   "source": [
    "# Calculate the count of product was totally ordered and how many times it was\n",
    "# reordered\n",
    "prod_stat = order_prior.groupby('product_id').agg(\n",
    "    {\n",
    "    'reordered': ['sum', 'size'],\n",
    "    'add_to_cart_order':'mean'\n",
    "    })\n",
    "\n",
    "# Set the column to level 1\n",
    "prod_stat.columns = prod_stat.columns.levels[1]\n",
    "# Rename the columns\n",
    "prod_stat.rename(columns={'sum':'prod_reorders',\n",
    "                          'size':'prod_orders',\n",
    "                          'mean': 'prod_add_to_card_mean'}, inplace=True)\n",
    "prod_stat.reset_index(inplace=True)\n",
    "\n",
    "prod_stat['reorder_ration'] = prod_stat['prod_reorders'] / prod_stat['prod_orders']\n",
    "\n",
    "prod_stat = pd.merge(prod_stat, prob, on='product_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjfLvofEFolD"
   },
   "source": [
    "## Calculate user statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "n2idy0jSFrqW"
   },
   "outputs": [],
   "source": [
    "# Get the max order_number for a given user_id\n",
    "# calculate the sum, mean, median of days_since_prior_order\n",
    "user_stat = orders[orders.eval_set == 'prior'].groupby('user_id').agg(\n",
    "    {\n",
    "        'order_number': 'max',\n",
    "        'days_since_prior_order': ['sum','mean','median']\n",
    "     })\n",
    "\n",
    "# Drop the 0th level of column generated from groupby\n",
    "user_stat.columns = user_stat.columns.droplevel(0)\n",
    "user_stat.rename(columns={'max': 'user_orders',\n",
    "                          'sum': 'user_order_starts_at',\n",
    "                          'mean': 'user_mean_days_since_prior',\n",
    "                          'median': 'user_median_days_since_prior'},\n",
    "                 inplace=True)\n",
    "\n",
    "user_stat.reset_index(inplace=True)\n",
    "\n",
    "# Merging the orders and prior orders to get the products info for orders\n",
    "orders_products = pd.merge(orders, order_prior, on=\"order_id\")\n",
    "\n",
    "# Compute the number of times user reordered till now and count of unique\n",
    "# products ordered by the user\n",
    "user_order_stat = orders_products.groupby('user_id').agg(\n",
    "    {\n",
    "        \"user_id\": \"size\",\n",
    "        \"reordered\": \"sum\",\n",
    "        \"product_id\": lambda x: x.nunique()\n",
    "    })\n",
    "\n",
    "user_order_stat.rename(\n",
    "    columns = {\n",
    "        'user_id':'user_total_products',\n",
    "        'product_id': 'user_distinct_products',\n",
    "        'reordered': 'user_reorder_ratio'\n",
    "    },\n",
    "    inplace=True)\n",
    "\n",
    "user_order_stat.reset_index(inplace=True)\n",
    "\n",
    "# compute the reorder ratio based on how many times the user has reordered till\n",
    "# now by total products ordered by the user\n",
    "user_order_stat['user_reorder_ratio'] = user_order_stat['user_reorder_ratio'] / user_order_stat['user_total_products']\n",
    "\n",
    "user_stat = pd.merge(user_stat, user_order_stat, on='user_id')\n",
    "# Calculate the avg basket size by total products bought and total user orders\n",
    "user_stat['user_average_basket'] = (user_stat['user_total_products'] \n",
    "                                    / user_stat['user_orders'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_H85d_mNTeO"
   },
   "source": [
    "## User product Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_9-JMzXNNVWi"
   },
   "outputs": [],
   "source": [
    "# How many users purchased a given product?\n",
    "prod_usr = orders_products.groupby(['product_id']).agg(\n",
    "    {'user_id': lambda x: x.nunique()})\n",
    "prod_usr.rename(columns={'user_id':'prod_users_unq'}, inplace=True)\n",
    "prod_usr.reset_index(inplace=True)\n",
    "\n",
    "# How many users reordered a product?\n",
    "prod_usr_reordered = orders_products[orders_products.reordered==True].groupby(\n",
    "    ['product_id']).agg(\n",
    "        {'user_id': lambda x: x.nunique()}\n",
    "        )\n",
    "prod_usr_reordered.rename(columns={'user_id': 'prod_users_unq_reordered'}, inplace=True)\n",
    "prod_usr_reordered.reset_index(inplace=True)\n",
    "\n",
    "order_stat = orders_products.groupby('order_id').agg(\n",
    "    {'order_id': 'size'})\n",
    "order_stat = order_stat.rename(columns={'order_id': 'order_size'}).reset_index()\n",
    "\n",
    "orders_products = pd.merge(orders_products, order_stat, on='order_id')\n",
    "orders_products['add_to_cart_order_inverted'] = orders_products['order_size'] - orders_products['add_to_cart_order']\n",
    "orders_products['add_to_cart_order_relative'] = orders_products['add_to_cart_order'] / orders_products['order_size']\n",
    "\n",
    "# Compute user product features related to orders\n",
    "data = orders_products.groupby(['user_id', 'product_id']).agg(\n",
    "    {\n",
    "        'user_id': 'size',\n",
    "        'order_number': ['min', 'max'],\n",
    "        'add_to_cart_order': ['mean', 'median'],\n",
    "        'days_since_prior_order': ['mean', 'median'],\n",
    "        'order_dow': ['mean', 'median'],\n",
    "        'order_hour_of_day': ['mean', 'median'],\n",
    "        'add_to_cart_order_inverted': ['mean', 'median'],\n",
    "        'add_to_cart_order_relative': ['mean', 'median'],\n",
    "        'reordered': ['sum']\n",
    "     })\n",
    "\n",
    "data.columns = data.columns.droplevel(0)\n",
    "data.columns = ['up_orders', 'up_first_order', 'up_last_order',\n",
    "                'up_mean_cart_position', 'up_median_cart_position',\n",
    "                'days_since_prior_order_mean',\n",
    "                'days_since_prior_order_median', 'order_dow_mean',\n",
    "                'order_dow_median', 'order_hour_of_day_mean', \n",
    "                'order_hour_of_day_median', 'add_to_cart_order_inverted_mean',\n",
    "                'add_to_cart_order_inverted_median',\n",
    "                'add_to_cart_order_relative_mean',\n",
    "                'add_to_cart_order_relative_median',\n",
    "                'reordered_sum']\n",
    "\n",
    "# Adding 1 incae of reordered_sum is 0\n",
    "data['user_product_reordered_ratio'] = (data['reordered_sum'] + 1.0) / data['up_orders']\n",
    "\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "data = pd.merge(data, prod_stat, on='product_id')\n",
    "data = pd.merge(data, user_stat, on='user_id')\n",
    "\n",
    "# Ratio of user ordered a particular product by total orders by the order\n",
    "data['up_order_rate'] = data['up_orders'] / data['user_orders']\n",
    "data['up_orders_since_last_order'] = data['user_orders'] - data['up_last_order']\n",
    "# ordering rate since first order by the user\n",
    "data['up_order_rate_since_first_order'] = data['user_orders'] / (data['user_orders'] - data['up_first_order'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the necessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'user_product_reordered_ratio', 'reordered_sum',\n",
    "    'add_to_cart_order_inverted_mean',\n",
    "    'add_to_cart_order_relative_mean', 'reorder_prob',\n",
    "    'last', 'prev1', 'prev2', 'median', 'mean',\n",
    "    'dep_reordered_ratio', 'aisle_reordered_ratio',\n",
    "    'aisle_products', 'aisle_reordered',\n",
    "    'dep_products', 'dep_reordered',\n",
    "    'prod_users_unq', 'prod_users_unq_reordered',\n",
    "    'order_number', 'prod_add_to_card_mean',\n",
    "    'days_since_prior_order',\n",
    "    'order_dow', 'order_hour_of_day',\n",
    "    'reorder_ration', 'user_orders',\n",
    "    'user_order_starts_at', 'user_mean_days_since_prior',\n",
    "    'user_average_basket', 'user_distinct_products',\n",
    "    'user_reorder_ratio', 'user_total_products',\n",
    "    'prod_orders', 'prod_reorders',\n",
    "    'up_order_rate', 'up_orders_since_last_order',\n",
    "    'up_order_rate_since_first_order',\n",
    "    'up_orders', 'up_first_order', 'up_last_order',\n",
    "    'up_mean_cart_position', 'days_since_prior_order_mean',\n",
    "    'order_dow_mean', 'order_hour_of_day_mean',\n",
    "    'user_id', 'order_id'\n",
    "    ]\n",
    "features.extend(embedings)\n",
    "categories = ['product_id', 'aisle_id', 'department_id']\n",
    "features.extend(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_features = features.copy()\n",
    "processed_features.remove('order_id')\n",
    "processed_features.remove('user_id')\n",
    "\n",
    "\n",
    "class CustomStackingClassifier:\n",
    "    \"\"\"\n",
    "    This Class accepts the estimator and params to train the models\n",
    "    The model is trained loop times inorder to perform bagging\n",
    "    \"\"\"\n",
    "    def __init__(self, estimators, random_state, params, nround, \n",
    "                 version, loop=3,\n",
    "                 valid_size=0.05, stratify=True, verbose=1,\n",
    "                 early_stopping=60, use_probas=True):\n",
    "        self.clf = estimators\n",
    "        self.mod=cpickle\n",
    "        self.loop = loop\n",
    "        self.params = params\n",
    "        self.nround = nround    \n",
    "        self.version = version\n",
    "        self.valid_size = valid_size\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.early_stopping = early_stopping\n",
    "        self.models = []\n",
    "\n",
    "\n",
    "    def split_build_valid(self, train_user, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Splits the dataset based on the user and divides them in train\n",
    "        and valid set\n",
    "        \"\"\"\n",
    "        train_user['is_valid'] = np.random.choice(\n",
    "            [0,1],\n",
    "            size=len(train_user),\n",
    "            p=[1-self.valid_size, self.valid_size])\n",
    "\n",
    "        valid_n = train_user['is_valid'].sum()\n",
    "        build_n = (train_user.shape[0] - valid_n)\n",
    "        \n",
    "        print('build user:{}, valid user:{}'.format(build_n, valid_n))\n",
    "        valid_user = train_user[train_user['is_valid']==1].user_id\n",
    "        is_valid = X_train.user_id.isin(valid_user)\n",
    "        \n",
    "        dbuild = lgb.Dataset(X_train[~is_valid].drop('user_id', axis=1),\n",
    "                             y_train[~is_valid],\n",
    "                             categorical_feature=['product_id', 'aisle_id', 'department_id'])\n",
    "        dvalid = lgb.Dataset(X_train[is_valid].drop('user_id', axis=1),\n",
    "                             label=y_train[is_valid],\n",
    "                             categorical_feature=['product_id', 'aisle_id', 'department_id'])\n",
    "        watchlist_set = [dbuild, dvalid]\n",
    "        watchlist_name = ['build', 'valid']\n",
    "        \n",
    "        print('FINAL SHAPE')\n",
    "        print('dbuild.shape:{}  dvalid.shape:{}\\n'.format(\n",
    "            dbuild.data.shape,\n",
    "            dvalid.data.shape))\n",
    "        return dbuild, dvalid, watchlist_set, watchlist_name\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        train_user = x[['user_id']].drop_duplicates()\n",
    "\n",
    "        for i in range(self.loop):\n",
    "            dbuild, dvalid, watchlist_set, watchlist_name = self.split_build_valid(train_user, x, y)\n",
    "            gc.collect();\n",
    "\n",
    "            # Train models\n",
    "            model = lgb.train(\n",
    "                self.params,\n",
    "                dbuild,\n",
    "                self.nround,\n",
    "                watchlist_set,\n",
    "                watchlist_name,\n",
    "                early_stopping_rounds=self.early_stopping,\n",
    "                categorical_feature=['product_id', 'aisle_id', 'department_id'],\n",
    "                verbose_eval=5)\n",
    "            joblib.dump(model, \"lgb_models/lgb_trained_{}_{}\".format(self.version, i))\n",
    "            self.models.append(model)\n",
    "            del [dbuild, dvalid, watchlist_set, watchlist_name];\n",
    "            gc.collect();\n",
    "        del train_user;\n",
    "        gc.collect()\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, x, test_data):\n",
    "#         dtest  = lgb.Dataset(x)\n",
    "        sub_test = test_data[['order_id', 'product_id']]\n",
    "        sub_test['yhat'] = 0\n",
    "        for model in self.models:\n",
    "            sub_test['yhat'] += model.predict(x)\n",
    "        sub_test['yhat'] /= self.loop\n",
    "        return sub_test\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "@author: Faron\n",
    "\"\"\"\n",
    "'''\n",
    "This kernel implements the O(n²) F1-Score expectation maximization algorithm presented in\n",
    "\"Ye, N., Chai, K., Lee, W., and Chieu, H.  Optimizing F-measures: A Tale of Two Approaches. In ICML, 2012.\"\n",
    "It solves argmax_(0 <= k <= n,[[None]]) E[F1(P,k,[[None]])]\n",
    "with [[None]] being the indicator for predicting label \"None\"\n",
    "given posteriors P = [p_1, p_2, ... , p_n], where p_1 > p_2 > ... > p_n\n",
    "under label independence assumption by means of dynamic programming in O(n²).\n",
    "'''\n",
    "class F1Optimizer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def get_expectations(P, pNone=None):\n",
    "        expectations = []\n",
    "        P = np.sort(P)[::-1]\n",
    "\n",
    "        n = np.array(P).shape[0]\n",
    "        DP_C = np.zeros((n + 2, n + 1))\n",
    "        if pNone is None:\n",
    "            pNone = (1.0 - P).prod()\n",
    "\n",
    "        DP_C[0][0] = 1.0\n",
    "        for j in range(1, n):\n",
    "            DP_C[0][j] = (1.0 - P[j - 1]) * DP_C[0, j - 1]\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            DP_C[i, i] = DP_C[i - 1, i - 1] * P[i - 1]\n",
    "            for j in range(i + 1, n + 1):\n",
    "                DP_C[i, j] = P[j - 1] * DP_C[i - 1, j - 1] + (1.0 - P[j - 1]) * DP_C[i, j - 1]\n",
    "\n",
    "        DP_S = np.zeros((2 * n + 1,))\n",
    "        DP_SNone = np.zeros((2 * n + 1,))\n",
    "        for i in range(1, 2 * n + 1):\n",
    "            DP_S[i] = 1. / (1. * i)\n",
    "            DP_SNone[i] = 1. / (1. * i + 1)\n",
    "        for k in range(n + 1)[::-1]:\n",
    "            f1 = 0\n",
    "            f1None = 0\n",
    "            for k1 in range(n + 1):\n",
    "                f1 += 2 * k1 * DP_C[k1][k] * DP_S[k + k1]\n",
    "                f1None += 2 * k1 * DP_C[k1][k] * DP_SNone[k + k1]\n",
    "            for i in range(1, 2 * k - 1):\n",
    "                DP_S[i] = (1 - P[k - 1]) * DP_S[i] + P[k - 1] * DP_S[i + 1]\n",
    "                DP_SNone[i] = (1 - P[k - 1]) * DP_SNone[i] + P[k - 1] * DP_SNone[i + 1]\n",
    "            expectations.append([f1None + 2 * pNone / (2 + k), f1])\n",
    "\n",
    "        return np.array(expectations[::-1]).T\n",
    "\n",
    "    @staticmethod\n",
    "    def maximize_expectation(P, pNone=None):\n",
    "        expectations = F1Optimizer.get_expectations(P, pNone)\n",
    "\n",
    "        ix_max = np.unravel_index(expectations.argmax(), expectations.shape)\n",
    "        max_f1 = expectations[ix_max]\n",
    "\n",
    "        predNone = True if ix_max[0] == 0 else False\n",
    "        best_k = ix_max[1]\n",
    "\n",
    "        return best_k, predNone, max_f1\n",
    "\n",
    "    @staticmethod\n",
    "    def _F1(tp, fp, fn):\n",
    "        return 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _Fbeta(tp, fp, fn, beta=1.0):\n",
    "        beta_squared = beta ** 2\n",
    "        return (1.0 + beta_squared) * tp / ((1.0 + beta_squared) * tp + fp + beta_squared * fn)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def get_best_prediction(items, preds, pNone=None):\n",
    "    items_preds = sorted(list(zip(items, preds)), key=itemgetter(1), reverse=True)\n",
    "    P = [p for i,p in items_preds]\n",
    "    L = [i for i,p in items_preds]\n",
    "    \n",
    "    opt = F1Optimizer.maximize_expectation(P, pNone)\n",
    "    best_prediction = ['None'] if opt[1] else []\n",
    "    best_prediction += (L[:opt[0]])\n",
    "    return ' '.join(list(map(str,best_prediction)))\n",
    "\n",
    "\n",
    "def preprocess_predict(user_id):\n",
    "    \"\"\"\n",
    "    The function accepts user_id and fetches the last order by that user.\n",
    "    Then gets the all the products purchased by the user till now.\n",
    "    Computes feature based on all the previous products purchased by the user\n",
    "    with all those features it is later passed to the predicted model and probability\n",
    "    of every product is calculated.\n",
    "    Later, these predictions are passed to F1Optimizer class which computes which products\n",
    "    are highly likely to be purchased and only return those\n",
    "    \"\"\"\n",
    "    # Get the user latest order\n",
    "    user_last_order = orders[orders['user_id']==user_id].tail(1)['order_id'].values[0]\n",
    "    # get the products purchased by the user till now\n",
    "    user_order = order_train_chunk[order_train_chunk['order_id'] == user_last_order]\n",
    "    user_order_products = user_order.copy()\n",
    "\n",
    "    # Generate the features\n",
    "    # Merge products\n",
    "    user_order = pd.merge(user_order, products, on='product_id')\n",
    "    # Merge the orders\n",
    "    user_order = pd.merge(user_order, orders, on='order_id')\n",
    "    # Merge the User department statistics\n",
    "    user_order = pd.merge(user_order, user_dep_stat, on=['user_id', 'department_id'])\n",
    "    # Merge the user aisle statistics\n",
    "    user_order = pd.merge(user_order, user_aisle_stat, on=['user_id', 'aisle_id'])\n",
    "    # Merge the user products features\n",
    "    user_order = pd.merge(user_order, prod_usr, on='product_id')\n",
    "    # Merge the user product reordered stats\n",
    "    user_order = pd.merge(user_order, prod_usr_reordered, on='product_id', how='left')\n",
    "    user_order.prod_users_unq_reordered.fillna(0, inplace=True)\n",
    "\n",
    "    # Merge the this prepared set with the data\n",
    "    user_order = pd.merge(user_order, data, on=['product_id', 'user_id'])\n",
    "\n",
    "    # Compute the aisle and departement reorder ratio\n",
    "    user_order['aisle_reordered_ratio'] = user_order['aisle_reordered'] / user_order['user_orders']\n",
    "    user_order['dep_reordered_ratio'] = user_order['dep_reordered'] / user_order['user_orders']\n",
    "\n",
    "    user_order = pd.merge(user_order, product_periods, on=['user_id', 'product_id'])\n",
    "    user_order = pd.merge(user_order, product_embeddings, on=['product_id'])\n",
    "    user_order = pd.merge(user_order, order_streaks, on=['user_id', 'product_id'], how='left')\n",
    "\n",
    "    # Get the important features\n",
    "    user_order = user_order[processed_features]\n",
    "\n",
    "    # Load the models\n",
    "    cscf_1 = CustomStackingClassifier(lgb, 71, None, 10000, 1)\n",
    "    cscf_1.models = [\n",
    "        joblib.load(\"lgb_models/lgb_trained_1_0\"),\n",
    "        joblib.load(\"lgb_models/lgb_trained_1_1\"),\n",
    "        joblib.load(\"lgb_models/lgb_trained_1_2\")\n",
    "    ]\n",
    "    cscf_2 = CustomStackingClassifier(lgb, 72, None, 10000, 2)\n",
    "    cscf_2.models = [\n",
    "        joblib.load(\"lgb_models/lgb_trained_2_0\"),\n",
    "        joblib.load(\"lgb_models/lgb_trained_2_1\"),\n",
    "        joblib.load(\"lgb_models/lgb_trained_2_2\")\n",
    "    ]\n",
    "    cscf_3 = CustomStackingClassifier(lgb, 73, None, 10000, 3)\n",
    "    cscf_3.models = [\n",
    "        joblib.load(\"lgb_models/lgb_trained_3_0\"),\n",
    "        joblib.load(\"lgb_models/lgb_trained_3_1\"),\n",
    "        joblib.load(\"lgb_models/lgb_trained_3_2\")\n",
    "    ]\n",
    "\n",
    "    # Predict with loaded models\n",
    "    predict_1 = cscf_1.predict(user_order, user_order_products)\n",
    "    predict_2 = cscf_2.predict(user_order, user_order_products)\n",
    "    predict_3 = cscf_3.predict(user_order, user_order_products)\n",
    "\n",
    "    # concat all the 3 predictions and compute mean\n",
    "    pred_item = pd.concat([predict_1, predict_2, predict_3])\n",
    "    pred_item = pred_item.groupby(['order_id','product_id']).yhat.mean().reset_index()\n",
    "\n",
    "    items = pred_item['product_id'].tolist()\n",
    "    preds = pred_item['yhat'].tolist()\n",
    "\n",
    "    predicted_products = get_best_prediction(items, preds)\n",
    "\n",
    "    return predicted_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items to be purchased by 9 are ['27973', '21462', '4957', '481', '41844', '40571', '42347', '26790', '13351', '5002', '42828', '6489', '16018', '43875', '38277', '3634', '38159', '12075', '8834']\n"
     ]
    }
   ],
   "source": [
    "user_id = 9\n",
    "products_predicted = preprocess_predict(user_id)\n",
    "print(f\"Items to be purchased by user {user_id} are {products_predicted.split()}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "train_and_test data generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
